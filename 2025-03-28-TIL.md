# GIT TIL 93 - ResNet(잔차신경망)의 개념과 구조

<br><br>


>ResNet(Residual Network, 잔차신경망)은 CNN 기반의 신경망 모델입니다.

<br><br>

## 1. 기존 딥러닝 네트워크와 ResNet의 차이

기존 딥러닝 네트워크와 ResNet의 가장 큰 차이점은 '잔차 학습(Residual Learning)' 방식에 있습니다.

![Image](https://github.com/user-attachments/assets/4dc60cef-feeb-4148-a7bc-1994b340ab31)

왼쪽은 일반적인 신경망 구조로, 입력 x를 받아 가중치 레이어와 활성화 함수를 거쳐 출력 H(x)를 생성합니다. <br><br>
그리고 오른쪽이 오늘 제가 배운 `ResNet` 구조입니다.


<br>

## 2. 심층 신경망의 문제점

### 깊어지는 신경망의 한계

이론적으로 신경망의 층을 깊게 쌓을수록 더 복잡한 특징을 학습할 수 있고 성능이 향상되어야 합니다.<br><br> 하지만 실제로는 일정 깊이 이상에서 오히려 성능이 저하되는 현상이 발생합니다.

<br>

- **기울기 소실(Gradient Vanishing)**: <br> 
 층이 깊어질수록 기울기가 0에 가까워져 앞쪽 층의 가중치가 거의 업데이트되지 않는 문제
- **기울기 폭주(Gradient Explosion)**: 반대로 미분값이 1보다 클 경우, 기울기가 너무 커져 학습이 불안정해지는 문제
- **표현력 저하**: 층이 깊어질수록 네트워크가 더 복잡한 함수를 표현해야 하지만, 학습 과정의 어려움으로 인해 표현력이 오히려 저하되는 현상

<br><br>

## 3. 기울기 소실 문제

신경망을 학습할 때는 오차를 줄이기 위해 가중치를 조금씩 바꾸는 작업을 반복합니다.<br><br>
이때 기울기(gradient)를 계산해서, 그 값을 따라 가중치를 조정하지만, 문제가 생기는 이유는 <br>
신경망이 깊어질수록, 기울기를 계속 곱하게 되는데<br><br>
이때 곱해지는 값들이 전부 1보다 작으면, 결국 거의 0에 가까운 숫자가 되어 학습이 사실상 중단됩니다.


<br>

## 4. ResNet의 핵심 - 숏컷 커넥션

### 숏컷 커넥션의 개념

ResNet의 핵심, 숏컷 커넥션(Shortcut Connection)은 입력 데이터를 몇 개의 층을 건너뛰어 출력에 직접 더해주는 방식입니다.

![Image](https://github.com/user-attachments/assets/52f1fe3e-47f7-4ff4-af83-727f1e4310d7)


<br><br>

## 4. 3D 평가 모델에서의 활용도

ResNet은 단순하면서도 강력한 아이디어를 통해 심층 신경망의 한계를 극복했습니다.<br><br> 잔차 연결이라는 방식으로 깊은 네트워크를 효과적으로 학습할 수 있게 되었고, 이는 현재 저희 프로젝트 3D 평가 모델에서도 활용되고 있습니다.

<br><br>

---

<br><br>
